{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b984fbae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-12T18:08:11.855081Z",
     "iopub.status.busy": "2024-01-12T18:08:11.854745Z",
     "iopub.status.idle": "2024-01-12T18:08:40.608111Z",
     "shell.execute_reply": "2024-01-12T18:08:40.607178Z"
    },
    "papermill": {
     "duration": 28.760657,
     "end_time": "2024-01-12T18:08:40.610301",
     "exception": false,
     "start_time": "2024-01-12T18:08:11.849644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287113"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train=pd.read_csv(\"/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/train.csv\")\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eab2919a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-12T18:08:40.619004Z",
     "iopub.status.busy": "2024-01-12T18:08:40.618692Z",
     "iopub.status.idle": "2024-01-12T18:08:40.740550Z",
     "shell.execute_reply": "2024-01-12T18:08:40.739693Z"
    },
    "papermill": {
     "duration": 0.128256,
     "end_time": "2024-01-12T18:08:40.742380",
     "exception": false,
     "start_time": "2024-01-12T18:08:40.614124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id            0\n",
       "article       0\n",
       "highlights    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afda1cda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-12T18:08:40.750698Z",
     "iopub.status.busy": "2024-01-12T18:08:40.750387Z",
     "iopub.status.idle": "2024-01-12T18:08:40.754602Z",
     "shell.execute_reply": "2024-01-12T18:08:40.753785Z"
    },
    "papermill": {
     "duration": 0.010486,
     "end_time": "2024-01-12T18:08:40.756514",
     "exception": false,
     "start_time": "2024-01-12T18:08:40.746028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training=train['article'][:2]\n",
    "testing=train['highlights'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c2b333d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-12T18:08:40.765013Z",
     "iopub.status.busy": "2024-01-12T18:08:40.764361Z",
     "iopub.status.idle": "2024-01-12T18:08:58.184806Z",
     "shell.execute_reply": "2024-01-12T18:08:58.183822Z"
    },
    "papermill": {
     "duration": 17.427213,
     "end_time": "2024-01-12T18:08:58.187309",
     "exception": false,
     "start_time": "2024-01-12T18:08:40.760096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tokenizer=tf.keras.preprocessing.text.Tokenizer()\n",
    "# random_sentence=\"12th Fail: Did you know IPS officer Manoj Kumar Sharma, IRS officer Shraddha Joshi had cameos in Vikrant Massey film?\"\n",
    "def tokenizenews(random_sentence):\n",
    "    tokenizer=tf.keras.preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts([random_sentence])\n",
    "    sequence = tokenizer.texts_to_sequences([random_sentence])\n",
    "    sequence_length = 50  # Adjust as needed\n",
    "    padded_sequence = tf.keras.preprocessing.sequence.pad_sequences(sequence, maxlen=sequence_length)\n",
    "    return padded_sequence\n",
    "training=training.apply(tokenizenews)\n",
    "testing=testing.apply(tokenizenews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c0ff779",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-12T18:08:58.196913Z",
     "iopub.status.busy": "2024-01-12T18:08:58.196051Z",
     "iopub.status.idle": "2024-01-12T18:08:58.204314Z",
     "shell.execute_reply": "2024-01-12T18:08:58.203583Z"
    },
    "papermill": {
     "duration": 0.015004,
     "end_time": "2024-01-12T18:08:58.206341",
     "exception": false,
     "start_time": "2024-01-12T18:08:58.191337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "def encoder(array):\n",
    "    vocab_size = 10000\n",
    "    embedding_dim = 200\n",
    "    encoder_hidden_states=[]\n",
    "    sequence_length = 50\n",
    "    input_sequence = tf.keras.Input(shape=(sequence_length, ))\n",
    "    embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_sequence)\n",
    "    encoder_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(1600 , return_sequences=True))(embedding_layer)\n",
    "    encoded_representation = tf.keras.layers.Concatenate()([encoder_lstm[:, -1, :], encoder_lstm[:, 0, :]])\n",
    "    model = tf.keras.Model(inputs=input_sequence, outputs=encoded_representation )\n",
    "    for random_sentence in array:\n",
    "        result = model.predict(random_sentence)\n",
    "        print(result.shape)\n",
    "        encoder_hidden_states.append(result)\n",
    "    return encoder_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d66ea7bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-12T18:08:58.214899Z",
     "iopub.status.busy": "2024-01-12T18:08:58.214642Z",
     "iopub.status.idle": "2024-01-12T18:08:58.230162Z",
     "shell.execute_reply": "2024-01-12T18:08:58.229399Z"
    },
    "papermill": {
     "duration": 0.022065,
     "end_time": "2024-01-12T18:08:58.232112",
     "exception": false,
     "start_time": "2024-01-12T18:08:58.210047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decoder(encoder_hidden_states,target_tokens,model1):\n",
    "    loss = 0\n",
    "    context_vector = None\n",
    "    decoder_state = None\n",
    "    sequence_length = 50\n",
    "    hidden_size = 128\n",
    "    embedding_dim = 128\n",
    "    vocab_size = 10000\n",
    "    context_vector=tf.zeros((128,50 ))\n",
    "    decoder_state=tf.zeros((128, 50))\n",
    "    learning_rate=0.001\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    v = tf.random.normal(shape=(hidden_size,50))\n",
    "    Wh = tf.random.normal(shape=(hidden_size, 50))\n",
    "    Ws = tf.random.normal(shape=(hidden_size, 50))\n",
    "    battn = tf.random.normal(shape=(hidden_size,50))\n",
    "    weights= tf.random.normal(shape=(100, 1))   \n",
    "    bias=tf.random.uniform([]) \n",
    "    for t in range(len(target_tokens)):\n",
    "        input_sequence = tf.keras.Input(shape=(sequence_length, ))\n",
    "        embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_sequence)\n",
    "#         LSTM=tf.keras.layers.LSTM(128,return_sequences=True)(embedding_layer)\n",
    "        model = tf.keras.Model(inputs=input_sequence, outputs=embedding_layer)\n",
    "        output_embeddings = model.predict(target_tokens[t])\n",
    "        attention_scores = []\n",
    "        for i in range(len(encoder_hidden_states)):\n",
    "            encoder_hidden_states[i] = tf.reshape(encoder_hidden_states[i], (128,50))\n",
    "            e_t_i = v * tf.nn.tanh(Wh * encoder_hidden_states[i]+ Ws*decoder_state+  battn)\n",
    "            attention_scores.append(e_t_i)\n",
    "        attention_distributon=tf.nn.softmax(attention_scores)\n",
    "        for i in range(len(encoder_hidden_states)):\n",
    "            context_vector += attention_distributon[i] * encoder_hidden_states[i]\n",
    "        output_embeddings=tf.reshape(output_embeddings, (128, 50, 1))\n",
    "        output_embeddings=tf.squeeze(output_embeddings, axis=-1)\n",
    "        concatenated_input = tf.expand_dims(tf.concat([context_vector,output_embeddings], axis=-1),axis=0)\n",
    "#         output_embeddings1 = model1.predict(concatenated_input)\n",
    "        output_embeddings1=tf.matmul(concatenated_input,weights)+bias\n",
    "        vocab_distribution=tf.nn.softmax(output_embeddings1,axis=1)\n",
    "        highest_prob_index=np.argmax(vocab_distribution)\n",
    "        target_word_prob = vocab_distribution[0,highest_prob_index,0]\n",
    "        timestep_loss = -tf.math.log(target_word_prob)\n",
    "        decoder_state=(decoder_state+context_vector)/2\n",
    "        decoder_state = tf.reshape(decoder_state, (128, 50))\n",
    "        print(timestep_loss)\n",
    "        loss += timestep_loss\n",
    "        meanloss=loss/(t+1)\n",
    "        #optimization process\n",
    "        v+=learning_rate*meanloss* tf.nn.tanh(Wh * encoder_hidden_states[i]+ Ws*decoder_state+  battn)\n",
    "        Ws+=learning_rate*meanloss*decoder_state\n",
    "        Wh=Wh+learning_rate*meanloss*encoder_hidden_states[i]\n",
    "        battn+=learning_rate*meanloss\n",
    "        bias+=learning_rate*meanloss\n",
    "        weights+=learning_rate*meanloss*weights\n",
    "        decoder_state=(decoder_state+context_vector)/2\n",
    "        decoder_state = tf.reshape(decoder_state, (128, 50))\n",
    "    return [v,Ws,Wh,battn,weights,bias]\n",
    "#         print(v,Ws,Wh,battn,decoder_state,model1.trainable_variables)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ffbeac0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-12T18:08:58.240418Z",
     "iopub.status.busy": "2024-01-12T18:08:58.240169Z",
     "iopub.status.idle": "2024-01-12T18:09:09.288490Z",
     "shell.execute_reply": "2024-01-12T18:09:09.287260Z"
    },
    "papermill": {
     "duration": 11.054889,
     "end_time": "2024-01-12T18:09:09.290699",
     "exception": false,
     "start_time": "2024-01-12T18:08:58.235810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "(1, 6400)\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "(1, 6400)\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "tf.Tensor(4.3535457, shape=(), dtype=float32)\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "tf.Tensor(4.3976393, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "encoder_hidden=encoder(training)\n",
    "# summarize=\"Germany, situated in central Europe, is renowned for its rich history, vibrant culture, and robust economy. With a population exceeding 80 million, it holds the highest population in the European Union. The country boasts picturesque landscapes like the Black Forest, Bavarian Alps, and Rhine Valley. Germany is a hub of technological advancements and innovation, housing top-tier universities and research institutions. It maintains well-developed infrastructure, efficient public transport, and a high standard of living. German cuisine, featuring hearty dishes like sausages, pretzels, and sauerkraut, is widely appreciated. Iconic landmarks such as the Brandenburg Gate, Neuschwanstein Castle, and Cologne Cathedral add to its allure. Offering a blend of historical sites and modern cities, Germany is a favored tourist destination and an attractive place for residence and employment.\"\n",
    "target_tokens=[tokenizenews(train['highlights'][0])]\n",
    "input_sequence1 = tf.keras.Input(shape=(128, 100))\n",
    "layer2=tf.keras.layers.Dense(1)(input_sequence1)\n",
    "model1 = tf.keras.Model(inputs=input_sequence1, outputs=layer2)\n",
    "v,Ws,Wh,battn,weights,bias=decoder(encoder_hidden,testing,model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1c2c040",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-12T18:09:09.302001Z",
     "iopub.status.busy": "2024-01-12T18:09:09.301456Z",
     "iopub.status.idle": "2024-01-12T18:09:09.319342Z",
     "shell.execute_reply": "2024-01-12T18:09:09.318509Z"
    },
    "papermill": {
     "duration": 0.025637,
     "end_time": "2024-01-12T18:09:09.321248",
     "exception": false,
     "start_time": "2024-01-12T18:09:09.295611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(para,v,Ws,Wh,battn,weights,bias,length=50):\n",
    "    tokenizer=tf.keras.preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts([para])\n",
    "    sequence = tokenizer.texts_to_sequences([para])\n",
    "    sequence_length = 50  # Adjust as needed\n",
    "    padded_sequence = tf.keras.preprocessing.sequence.pad_sequences(sequence, maxlen=sequence_length)\n",
    "    tokenspara=tokenizenews(para)\n",
    "    print(tokenspara)\n",
    "    encoder_hidden_states=encoder([tokenspara])\n",
    "    summary=[]\n",
    "    sequence_length = 50\n",
    "    hidden_size = 128\n",
    "    embedding_dim = 50\n",
    "    vocab_size = 10000\n",
    "    context_vector=tf.zeros((128,50 ))\n",
    "    decoder_state=tf.zeros((128, 50))\n",
    "    current_token = np.zeros((128,))\n",
    "    for j in range(length):\n",
    "        input_sequence = tf.keras.Input(shape=(sequence_length, ))\n",
    "        embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_sequence)\n",
    "#         LSTM=tf.keras.layers.LSTM(128,return_sequences=True)(embedding_layer)\n",
    "        model = tf.keras.Model(inputs=input_sequence, outputs=embedding_layer)\n",
    "        output_embeddings = model.predict(current_token)\n",
    "        attention_scores = []\n",
    "        for i in range(len(encoder_hidden_states)):\n",
    "            encoder_hidden_states[i] = tf.reshape(encoder_hidden_states[i], (128,50))\n",
    "            e_t_i = v * tf.nn.tanh(Wh * encoder_hidden_states[i]+ Ws*decoder_state+  battn)\n",
    "            attention_scores.append(e_t_i)\n",
    "        attention_distributon=tf.nn.softmax(attention_scores)\n",
    "        for i in range(len(encoder_hidden_states)):\n",
    "            context_vector += attention_distributon[i] * encoder_hidden_states[i]\n",
    "#         print(output_embeddings.shape)\n",
    "        output_embeddings=tf.reshape(output_embeddings, (128, 50, 1))\n",
    "        output_embeddings=tf.squeeze(output_embeddings, axis=-1)\n",
    "        concatenated_input = tf.expand_dims(tf.concat([context_vector,output_embeddings], axis=-1),axis=0)\n",
    "#         output_embeddings1 = model1.predict(concatenated_input)\n",
    "        output_embeddings1=tf.matmul(concatenated_input,weights)+bias\n",
    "        vocab_distribution=tf.nn.softmax(output_embeddings1,axis=1)\n",
    "        highest_prob_index=np.argmax(vocab_distribution)\n",
    "        current_token[j]=highest_prob_index\n",
    "#         print(highest_prob_index)\n",
    "        decoder_state=(decoder_state+context_vector)/2\n",
    "        decoder_state = tf.reshape(decoder_state, (128, 50))\n",
    "        summary.append(current_token)\n",
    "    tokentoword={tokenizer.word_index[i]:i for i in tokenizer.word_index.keys()}\n",
    "#     print(tokentoword)\n",
    "    return (\" \".join([tokentoword[i]if i!=0 else \" \" for i in current_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5262a85a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-12T18:09:09.331889Z",
     "iopub.status.busy": "2024-01-12T18:09:09.331643Z",
     "iopub.status.idle": "2024-01-12T18:09:26.896409Z",
     "shell.execute_reply": "2024-01-12T18:09:26.895519Z"
    },
    "papermill": {
     "duration": 17.57232,
     "end_time": "2024-01-12T18:09:26.898356",
     "exception": false,
     "start_time": "2024-01-12T18:09:09.326036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 62 209 210 211 212   2 213   3 214 215   1 216  41  35  15 217  36 218\n",
      "   46 219 220   2  56  34 221   3  33 222  22   7   1 223  24   9   4 224\n",
      "  225  20  14  10   2  34  12 226  17   9   4  13  18  11]]\n",
      "1/1 [==============================] - 1s 686ms/step\n",
      "(1, 6400)\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "extra   extra extra his extra extra time time time time extra time team time time during mascherano mascherano time before extra time sabella before time mascherano argentina mascherano extra before argentina's led holland out out argentina's up holland time out during right over up mates leader quiet argentina's leader video before before extra follow before during argentina video time with penalties match leader mascherano shoot out before before before mascherano match extra of out up with with paulo with before paulo mascherano leader paulo 90 time of video who mascherano of leader who who who quiet argentina match intently                                                        \n"
     ]
    }
   ],
   "source": [
    "print(test(train['article'][100], v,Ws,Wh,battn,weights,bias,100))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1654566,
     "sourceId": 2734496,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 83.371763,
   "end_time": "2024-01-12T18:09:30.647584",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-12T18:08:07.275821",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
