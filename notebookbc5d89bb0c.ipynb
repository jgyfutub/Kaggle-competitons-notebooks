{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import BertTokenizer, TFBertModel\nimport pandas as pd\ntrain=pd.read_csv(\"/kaggle/input/contradictory-my-dear-watson/train.csv\")\nprint(train.head())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-20T13:33:00.082997Z","iopub.execute_input":"2023-08-20T13:33:00.083347Z","iopub.status.idle":"2023-08-20T13:33:00.152479Z","shell.execute_reply.started":"2023-08-20T13:33:00.083320Z","shell.execute_reply":"2023-08-20T13:33:00.151310Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"           id                                            premise  \\\n0  5130fd2cb5  and these comments were considered in formulat...   \n1  5b72532a0b  These are issues that we wrestle with in pract...   \n2  3931fbe82a  Des petites choses comme celles-là font une di...   \n3  5622f0c60b  you know they can't really defend themselves l...   \n4  86aaa48b45  ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสด...   \n\n                                          hypothesis lang_abv language  label  \n0  The rules developed in the interim were put to...       en  English      0  \n1  Practice groups are not permitted to work on t...       en  English      2  \n2              J'essayais d'accomplir quelque chose.       fr   French      0  \n3  They can't defend themselves because of their ...       en  English      0  \n4    เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร       th     Thai      1  \n","output_type":"stream"}]},{"cell_type":"code","source":"model_name = 'bert-base-multilingual-cased'\ntokenizer = BertTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T13:33:00.154050Z","iopub.execute_input":"2023-08-20T13:33:00.154325Z","iopub.status.idle":"2023-08-20T13:33:00.419834Z","shell.execute_reply.started":"2023-08-20T13:33:00.154301Z","shell.execute_reply":"2023-08-20T13:33:00.418823Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def encode(line):\n    listtokens=list(tokenizer.tokenize(line))\n    listtokens.append(\"anthim\")\n    return tokenizer.convert_tokens_to_ids(listtokens)\nprint(encode(\"I love Shagun\"))","metadata":{"execution":{"iopub.status.busy":"2023-08-20T13:33:00.421162Z","iopub.execute_input":"2023-08-20T13:33:00.421475Z","iopub.status.idle":"2023-08-20T13:33:00.427744Z","shell.execute_reply.started":"2023-08-20T13:33:00.421448Z","shell.execute_reply":"2023-08-20T13:33:00.426739Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"[146, 16138, 106583, 32657, 100]\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\n\ndef encode_sentences(premises,hypothesis,tokenizer):\n    hypothesis=tf.ragged.constant([\n        encode(s) for s in np.array(hypothesis)  ])\n    premises=tf.ragged.constant([\n        encode(s) for s in np.array(premises)  ])\n    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*hypothesis.shape[0]\n    input_word_ids = tf.concat([cls, hypothesis,premises], axis=-1)\n\n    input_mask = tf.ones_like(input_word_ids).to_tensor()\n\n    type_cls = tf.zeros_like(cls)\n    type_s1 = tf.zeros_like(hypothesis)\n    type_s2 = tf.ones_like(premises)\n    input_type_ids = tf.concat(\n      [type_cls, type_s1, type_s2], axis=-1).to_tensor()\n\n    inputs = {\n        'input_word_ids': input_word_ids.to_tensor(),\n      'input_mask': input_mask,\n      'input_type_ids': input_type_ids}\n\n    return inputs","metadata":{"execution":{"iopub.status.busy":"2023-08-20T13:35:46.843016Z","iopub.execute_input":"2023-08-20T13:35:46.844275Z","iopub.status.idle":"2023-08-20T13:35:46.852589Z","shell.execute_reply.started":"2023-08-20T13:35:46.844237Z","shell.execute_reply":"2023-08-20T13:35:46.851515Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"train_input = encode_sentences(train.premise.values, train.hypothesis.values, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T13:35:50.292844Z","iopub.execute_input":"2023-08-20T13:35:50.293221Z","iopub.status.idle":"2023-08-20T13:36:07.001687Z","shell.execute_reply.started":"2023-08-20T13:35:50.293192Z","shell.execute_reply":"2023-08-20T13:36:07.000704Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"max_len=259\ndef build_model():\n    bert_encoder = TFBertModel.from_pretrained(model_name)\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_type_ids\")\n    \n    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])\n    \n    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-08-20T13:37:40.163178Z","iopub.execute_input":"2023-08-20T13:37:40.163549Z","iopub.status.idle":"2023-08-20T13:37:40.172410Z","shell.execute_reply.started":"2023-08-20T13:37:40.163522Z","shell.execute_reply":"2023-08-20T13:37:40.171293Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"model = build_model()\nmodel.summary()\nmodel.fit(train_input, train.label.values, epochs = 2, verbose = 1, batch_size = 64, validation_split = 0.2)","metadata":{"execution":{"iopub.status.busy":"2023-08-20T13:38:21.053023Z","iopub.execute_input":"2023-08-20T13:38:21.053440Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7767b8a69954b7e9d9eae1fdc6649cb"}},"metadata":{}},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_word_ids (InputLayer)    [(None, 259)]        0           []                               \n                                                                                                  \n input_mask (InputLayer)        [(None, 259)]        0           []                               \n                                                                                                  \n input_type_ids (InputLayer)    [(None, 259)]        0           []                               \n                                                                                                  \n tf_bert_model (TFBertModel)    TFBaseModelOutputWi  177853440   ['input_word_ids[0][0]',         \n                                thPoolingAndCrossAt               'input_mask[0][0]',             \n                                tentions(last_hidde               'input_type_ids[0][0]']         \n                                n_state=(None, 259,                                               \n                                 768),                                                            \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 past_key_values=No                                               \n                                ne, hidden_states=N                                               \n                                one, attentions=Non                                               \n                                e, cross_attentions                                               \n                                =None)                                                            \n                                                                                                  \n tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n ingOpLambda)                                                                                     \n                                                                                                  \n dense (Dense)                  (None, 3)            2307        ['tf.__operators__.getitem[0][0]'\n                                                                 ]                                \n                                                                                                  \n==================================================================================================\nTotal params: 177,855,747\nTrainable params: 177,855,747\nNon-trainable params: 0\n__________________________________________________________________________________________________\nEpoch 1/2\n","output_type":"stream"}]}]}