{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":1926230,"datasetId":1148896,"databundleVersionId":1964789}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nimport numpy as np\nimport re\ndataset=pd.read_csv('../input/en-fr-translation-dataset/en-fr.csv')\ndataset.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-25T16:39:23.085214Z","iopub.execute_input":"2024-05-25T16:39:23.085693Z","iopub.status.idle":"2024-05-25T16:43:29.977921Z","shell.execute_reply.started":"2024-05-25T16:39:23.085647Z","shell.execute_reply":"2024-05-25T16:43:29.976885Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                                  en  \\\n0  Changing Lives | Changing Society | How It Wor...   \n1                                           Site map   \n2                                           Feedback   \n3                                            Credits   \n4                                           Français   \n\n                                                  fr  \n0  Il a transformé notre vie | Il a transformé la...  \n1                                       Plan du site  \n2                                        Rétroaction  \n3                                            Crédits  \n4                                            English  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>fr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Changing Lives | Changing Society | How It Wor...</td>\n      <td>Il a transformé notre vie | Il a transformé la...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Site map</td>\n      <td>Plan du site</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Feedback</td>\n      <td>Rétroaction</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Credits</td>\n      <td>Crédits</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Français</td>\n      <td>English</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def english_preprocessing(data , col) : \n    data[col] = data[col].astype(str) \n    data[col] = data[col].apply(lambda x: x.lower())\n    data[col] = data[col].apply(lambda x: re.sub(\"[^A-Za-z\\s]\",\"\",x)) \n    data[col] = data[col].apply(lambda x: x.replace(\"\\s+\",\" \"))\n    data[col] = data[col].apply(lambda x: \" \".join([word for word in x.split()]))\n    return data \ndef french_preprocessing(data , col) : \n    data[col] = data[col].astype(str) \n    data[col] = data[col].apply(lambda x : x.lower()) \n    data[col] = data[col].apply(lambda x: re.sub(r'\\d','',x))\n    data[col] = data[col].apply(lambda x: re.sub(r'\\s+',' ',x))\n    data[col] = data[col].apply(lambda x: re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,।]\", \"\", x))\n    data[col] = data[col].apply(lambda x: x.strip()) \n    data[col] = \"<sos> \" + data[col] + \" <eos>\" \n    return data","metadata":{"execution":{"iopub.status.busy":"2024-05-25T16:43:29.980473Z","iopub.execute_input":"2024-05-25T16:43:29.981033Z","iopub.status.idle":"2024-05-25T16:43:29.992889Z","shell.execute_reply.started":"2024-05-25T16:43:29.980993Z","shell.execute_reply":"2024-05-25T16:43:29.991942Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from collections import Counter \ndef tokenizer(col):\n    if col=='en':\n        sents = english_preprocessing(dataset[:100] , col)[col].tolist()  \n    elif col=='fr':\n         sents = french_preprocessing(dataset[:100] , col)[col].tolist()  \n    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=100 , oov_token = \"<OOV>\" , filters='!#$%&()*+,-/:;<=>@«»\"\"[\\\\]^_`{|}~\\t\\n')\n    tokenizer.fit_on_texts(sents) \n    tokenizer.word_index['<pad>'] = 0 \n    tokenizer.index_word[0] = '<pad>' \n    vocab_to_idx = tokenizer.word_index \n    idx_to_vocab = tokenizer.index_word \n    seqs = tokenizer.texts_to_sequences(sents)  \n    pad_seqs = tf.keras.preprocessing.sequence.pad_sequences(seqs , maxlen =100 , padding='post')\n    return vocab_to_idx , idx_to_vocab , pad_seqs , tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-05-25T16:43:29.994074Z","iopub.execute_input":"2024-05-25T16:43:29.994337Z","iopub.status.idle":"2024-05-25T16:43:30.005886Z","shell.execute_reply.started":"2024-05-25T16:43:29.994305Z","shell.execute_reply":"2024-05-25T16:43:30.004740Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"en_vocab , en_inv_vocab , en_seqs , en_tokenizer = tokenizer('en')\nfr_vocab , fr_inv_vocab , fr_seqs , fr_tokenizer = tokenizer('fr')","metadata":{"execution":{"iopub.status.busy":"2024-05-25T16:43:30.007046Z","iopub.execute_input":"2024-05-25T16:43:30.007377Z","iopub.status.idle":"2024-05-25T16:43:30.039647Z","shell.execute_reply.started":"2024-05-25T16:43:30.007351Z","shell.execute_reply":"2024-05-25T16:43:30.038776Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/386876738.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data[col] = data[col].astype(str)\n/tmp/ipykernel_34/386876738.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data[col] = data[col].apply(lambda x: x.lower())\n/tmp/ipykernel_34/386876738.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data[col] = data[col].apply(lambda x: re.sub(\"[^A-Za-z\\s]\",\"\",x))\n/tmp/ipykernel_34/386876738.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data[col] = data[col].apply(lambda x: x.replace(\"\\s+\",\" \"))\n/tmp/ipykernel_34/386876738.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data[col] = data[col].apply(lambda x: \" \".join([word for word in x.split()]))\n/tmp/ipykernel_34/386876738.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data[col] = data[col].astype(str)\n/tmp/ipykernel_34/386876738.py:10: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data[col] = data[col].apply(lambda x : x.lower())\n/tmp/ipykernel_34/386876738.py:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data[col] = data[col].apply(lambda x: re.sub(r'\\d','',x))\n/tmp/ipykernel_34/386876738.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data[col] = data[col].apply(lambda x: re.sub(r'\\s+',' ',x))\n/tmp/ipykernel_34/386876738.py:13: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data[col] = data[col].apply(lambda x: re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,।]\", \"\", x))\n/tmp/ipykernel_34/386876738.py:14: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data[col] = data[col].apply(lambda x: x.strip())\n/tmp/ipykernel_34/386876738.py:15: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data[col] = \"<sos> \" + data[col] + \" <eos>\"\n","output_type":"stream"}]},{"cell_type":"code","source":"model = tf.keras.Sequential([ tf.keras.layers.Embedding(input_dim=10000 ,output_dim=768, input_length=100)])\ndef embedder(text):\n    global model\n    cls_embedding = model(text)\n    #Positional encoding \n    seq_len,d,n=100,768,10000\n    P = np.zeros((seq_len, d))\n    for k in range(seq_len):\n        for i in np.arange(int(d/2)):\n            denominator = np.power(n, 2*i/d)\n            P[k, 2*i] = np.sin(k/denominator)\n            P[k, 2*i+1] = np.cos(k/denominator)\n    #Adding positional encoding\n    cls_embedding += P\n    return tf.expand_dims(cls_embedding, axis=0)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-25T16:43:30.041858Z","iopub.execute_input":"2024-05-25T16:43:30.042124Z","iopub.status.idle":"2024-05-25T16:43:30.055771Z","shell.execute_reply.started":"2024-05-25T16:43:30.042101Z","shell.execute_reply":"2024-05-25T16:43:30.054845Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:86: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"#encoder\nWqe=tf.random.normal(shape=(1,100, 768))\nWke=tf.random.normal(shape=(1,100, 768))\nWve=tf.random.normal(shape=(1,100, 768))\nW0e=tf.random.normal(shape=(1,100, 768))\nW1e=tf.random.normal(shape=(1,100, 768))\n#decoder\nWqd=tf.random.normal(shape=(1,100, 768))\nWkd=tf.random.normal(shape=(1,100, 768))\nWvd=tf.random.normal(shape=(1,100 ,768))\nW0d=tf.random.normal(shape=(1,100 ,768))\nW1d=tf.random.normal(shape=(1,100 ,768))\nW01d=tf.random.normal(shape=(1,100, 768))\nW11d=tf.random.normal(shape=(1,100, 768))\n#train\nW0t=tf.random.normal(shape=(1,100, 768))","metadata":{"execution":{"iopub.status.busy":"2024-05-25T16:43:30.057126Z","iopub.execute_input":"2024-05-25T16:43:30.057776Z","iopub.status.idle":"2024-05-25T16:43:30.729908Z","shell.execute_reply.started":"2024-05-25T16:43:30.057743Z","shell.execute_reply":"2024-05-25T16:43:30.728882Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import math\ndef encoder(embed):\n    # weights of query ,keys and values and other weights\n    global Wqe,Wke,Wve,W0e,W1e\n    #Calculating query ,keys and values\n    Query=embed*Wqe\n    Key=embed*Wke\n    Value=embed*Wve\n    # Scaled-Dot Product Attention\n    scores = tf.matmul(Query, Key, transpose_b=True) / math.sqrt(tf.cast(768, tf.float32))\n    weights = tf.keras.activations.softmax(scores)\n    result=tf.matmul(weights, Value)\n    #Add layer\n    embed+=result\n    #normalize the embeddings\n    normalized_embeddings = tf.keras.layers.LayerNormalization()(embed)\n    #feed forward layer from scratch\n    Layer1=W0e*normalized_embeddings\n    result1=tf.nn.relu(Layer1)\n    result=W1e*normalized_embeddings\n    #Add layer\n    embed+=result\n    #normalize the embeddings\n    normalized_embeddings = tf.keras.layers.LayerNormalization()(embed)\n    Key=normalized_embeddings*Wke\n    Value=normalized_embeddings*Wve\n    return [Key,Value]","metadata":{"execution":{"iopub.status.busy":"2024-05-25T16:43:30.731300Z","iopub.execute_input":"2024-05-25T16:43:30.731938Z","iopub.status.idle":"2024-05-25T16:43:30.740312Z","shell.execute_reply.started":"2024-05-25T16:43:30.731904Z","shell.execute_reply":"2024-05-25T16:43:30.739360Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def decoder(embed, arr):\n    # weights of query ,keys and values and other weights\n    global Wqd,Wkd,Wvd,W0d,W1d,W01d,W11d\n    #Calculating query ,keys and values\n    Query=embed*Wqd\n    Key=embed*Wkd\n    Value=embed*Wvd\n    # Scaled-Dot Product Attention\n    scores = tf.matmul(Query, Key, transpose_b=True) / math.sqrt(tf.cast(768, tf.float32))\n    weights = tf.keras.activations.softmax(scores)\n    result=tf.matmul(weights, Value)\n    #Add layer\n    embed+=result\n    #normalize the embeddings\n    normalized_embeddings = tf.keras.layers.LayerNormalization()(embed)\n    #feed forward layer from scratch\n    Layer1=W0d*normalized_embeddings\n    result1=tf.nn.relu(Layer1)\n    result=W1d*normalized_embeddings\n    #Add layer\n    embed+=result\n    #normalize the embeddings\n    embed = tf.keras.layers.LayerNormalization()(embed)\n    #key ,value of encodings output\n    Key,Value=arr\n    Query=embed*Wqd\n    scores = tf.matmul(Query, Key, transpose_b=True) / math.sqrt(tf.cast(768, tf.float32))\n    weights = tf.keras.activations.softmax(scores)\n    result=tf.matmul(weights, Value)\n    #Add layer\n    embed+=result\n    #normalize the embeddings\n    normalized_embeddings = tf.keras.layers.LayerNormalization()(embed)\n    #feed forward layer from scratch\n    Layer1=W01d*normalized_embeddings\n    result1=tf.nn.relu(Layer1)\n    result=W11d*normalized_embeddings\n    #Add layer\n    embed+=result\n    return embed","metadata":{"execution":{"iopub.status.busy":"2024-05-25T16:43:30.741355Z","iopub.execute_input":"2024-05-25T16:43:30.741657Z","iopub.status.idle":"2024-05-25T16:43:30.756436Z","shell.execute_reply.started":"2024-05-25T16:43:30.741634Z","shell.execute_reply":"2024-05-25T16:43:30.755648Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def train(src , trg,opt):\n    global model,tokenizer,W0t,Wqe,Wke,Wve,W0e,W1e,Wqd,Wkd,Wvd,W0d,W1d,W01d,W11d\n    encoder_output=encoder(embedder(src))\n    output_embed=embedder(trg)\n    decoder_output=decoder(output_embed,encoder_output)\n    Layer1=W0t*decoder_output\n    result=tf.nn.softmax(Layer1, axis=-1)\n    predicted_id = tf.cast(tf.argmax(result, axis=-1), tf.int64) \n    pred_sent = ' '.join([fr_tokenizer.index_word[idx] for idx in predicted_id[0].numpy() if idx != 0 and idx != 2 and idx !=3 and idx in fr_tokenizer.index_word.keys() ])\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=output_embed, logits=result))\n    print(loss.numpy())\n    #backpropagation (just for now)\n    W0t+=loss.numpy()*opt\n    Wqe+=loss.numpy()*opt\n    Wke+=loss.numpy()*opt\n    Wve+=loss.numpy()*opt\n    W0e+=loss.numpy()*opt\n    W1e+=loss.numpy()*opt\n    Wqd+=loss.numpy()*opt\n    Wkd+=loss.numpy()*opt\n    Wvd+=loss.numpy()*opt\n    W0d+=loss.numpy()*opt\n    W1d+=loss.numpy()*opt\n    W01d+=loss.numpy()*opt\n    W11d+=loss.numpy()*opt\n    return loss","metadata":{"execution":{"iopub.status.busy":"2024-05-25T16:56:42.654758Z","iopub.execute_input":"2024-05-25T16:56:42.655430Z","iopub.status.idle":"2024-05-25T16:56:42.665595Z","shell.execute_reply.started":"2024-05-25T16:56:42.655393Z","shell.execute_reply":"2024-05-25T16:56:42.664532Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"train_set = tf.data.Dataset.from_tensor_slices((en_seqs , fr_seqs ))\ntrain_set = train_set.shuffle(100).batch(15 , drop_remainder = True)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T16:43:30.771318Z","iopub.execute_input":"2024-05-25T16:43:30.771600Z","iopub.status.idle":"2024-05-25T16:43:30.795668Z","shell.execute_reply.started":"2024-05-25T16:43:30.771578Z","shell.execute_reply":"2024-05-25T16:43:30.794767Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm \ndef trainloop(EPOCHS,optimizer):\n    for epoch in tqdm(range(EPOCHS)) :\n        for src , trg in tqdm(train_set) : \n            for i in range(15):\n                print(train(src[i],trg[i],optimizer))\ntrainloop(1,0.0001)","metadata":{"execution":{"iopub.status.busy":"2024-05-25T16:56:45.084133Z","iopub.execute_input":"2024-05-25T16:56:45.084815Z","iopub.status.idle":"2024-05-25T16:57:52.073619Z","shell.execute_reply.started":"2024-05-25T16:56:45.084784Z","shell.execute_reply":"2024-05-25T16:57:52.072619Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa03c0c166dc430197777bfd24e16e93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77db10f5a337401fa13667de77e6e3ab"}},"metadata":{}},{"name":"stdout","text":"1827.7269\ntf.Tensor(1827.7269, shape=(), dtype=float32)\n1827.756\ntf.Tensor(1827.756, shape=(), dtype=float32)\n1827.5547\ntf.Tensor(1827.5547, shape=(), dtype=float32)\n1827.9775\ntf.Tensor(1827.9775, shape=(), dtype=float32)\n1827.7852\ntf.Tensor(1827.7852, shape=(), dtype=float32)\n1827.7985\ntf.Tensor(1827.7985, shape=(), dtype=float32)\n1827.875\ntf.Tensor(1827.875, shape=(), dtype=float32)\n1827.812\ntf.Tensor(1827.812, shape=(), dtype=float32)\n1828.0078\ntf.Tensor(1828.0078, shape=(), dtype=float32)\n1827.841\ntf.Tensor(1827.841, shape=(), dtype=float32)\n1827.7812\ntf.Tensor(1827.7812, shape=(), dtype=float32)\n1828.2572\ntf.Tensor(1828.2572, shape=(), dtype=float32)\n1828.0353\ntf.Tensor(1828.0353, shape=(), dtype=float32)\n1827.6812\ntf.Tensor(1827.6812, shape=(), dtype=float32)\n1828.0063\ntf.Tensor(1828.0063, shape=(), dtype=float32)\n1827.9689\ntf.Tensor(1827.9689, shape=(), dtype=float32)\n1827.8912\ntf.Tensor(1827.8912, shape=(), dtype=float32)\n1828.2812\ntf.Tensor(1828.2812, shape=(), dtype=float32)\n1827.7053\ntf.Tensor(1827.7053, shape=(), dtype=float32)\n1827.7255\ntf.Tensor(1827.7255, shape=(), dtype=float32)\n1827.7883\ntf.Tensor(1827.7883, shape=(), dtype=float32)\n1827.82\ntf.Tensor(1827.82, shape=(), dtype=float32)\n1827.8665\ntf.Tensor(1827.8665, shape=(), dtype=float32)\n1827.7837\ntf.Tensor(1827.7837, shape=(), dtype=float32)\n1827.7859\ntf.Tensor(1827.7859, shape=(), dtype=float32)\n1827.5769\ntf.Tensor(1827.5769, shape=(), dtype=float32)\n1827.5206\ntf.Tensor(1827.5206, shape=(), dtype=float32)\n1828.0771\ntf.Tensor(1828.0771, shape=(), dtype=float32)\n1827.7493\ntf.Tensor(1827.7493, shape=(), dtype=float32)\n1827.8269\ntf.Tensor(1827.8269, shape=(), dtype=float32)\n1827.9968\ntf.Tensor(1827.9968, shape=(), dtype=float32)\n1827.7885\ntf.Tensor(1827.7885, shape=(), dtype=float32)\n1827.8997\ntf.Tensor(1827.8997, shape=(), dtype=float32)\n1827.8103\ntf.Tensor(1827.8103, shape=(), dtype=float32)\n1827.875\ntf.Tensor(1827.875, shape=(), dtype=float32)\n1828.2253\ntf.Tensor(1828.2253, shape=(), dtype=float32)\n1827.7194\ntf.Tensor(1827.7194, shape=(), dtype=float32)\n1828.136\ntf.Tensor(1828.136, shape=(), dtype=float32)\n1828.0815\ntf.Tensor(1828.0815, shape=(), dtype=float32)\n1827.7963\ntf.Tensor(1827.7963, shape=(), dtype=float32)\n1827.8386\ntf.Tensor(1827.8386, shape=(), dtype=float32)\n1827.82\ntf.Tensor(1827.82, shape=(), dtype=float32)\n1827.6626\ntf.Tensor(1827.6626, shape=(), dtype=float32)\n1828.0847\ntf.Tensor(1828.0847, shape=(), dtype=float32)\n1827.6362\ntf.Tensor(1827.6362, shape=(), dtype=float32)\n1827.8718\ntf.Tensor(1827.8718, shape=(), dtype=float32)\n1828.1227\ntf.Tensor(1828.1227, shape=(), dtype=float32)\n1827.9263\ntf.Tensor(1827.9263, shape=(), dtype=float32)\n1827.9285\ntf.Tensor(1827.9285, shape=(), dtype=float32)\n1827.7189\ntf.Tensor(1827.7189, shape=(), dtype=float32)\n1827.8081\ntf.Tensor(1827.8081, shape=(), dtype=float32)\n1827.984\ntf.Tensor(1827.984, shape=(), dtype=float32)\n1827.8955\ntf.Tensor(1827.8955, shape=(), dtype=float32)\n1827.7935\ntf.Tensor(1827.7935, shape=(), dtype=float32)\n1827.8844\ntf.Tensor(1827.8844, shape=(), dtype=float32)\n1827.8816\ntf.Tensor(1827.8816, shape=(), dtype=float32)\n1827.7887\ntf.Tensor(1827.7887, shape=(), dtype=float32)\n1827.5936\ntf.Tensor(1827.5936, shape=(), dtype=float32)\n1827.9744\ntf.Tensor(1827.9744, shape=(), dtype=float32)\n1827.818\ntf.Tensor(1827.818, shape=(), dtype=float32)\n1828.0498\ntf.Tensor(1828.0498, shape=(), dtype=float32)\n1828.3668\ntf.Tensor(1828.3668, shape=(), dtype=float32)\n1828.0732\ntf.Tensor(1828.0732, shape=(), dtype=float32)\n1827.8022\ntf.Tensor(1827.8022, shape=(), dtype=float32)\n1827.9197\ntf.Tensor(1827.9197, shape=(), dtype=float32)\n1827.726\ntf.Tensor(1827.726, shape=(), dtype=float32)\n1827.8129\ntf.Tensor(1827.8129, shape=(), dtype=float32)\n1827.6533\ntf.Tensor(1827.6533, shape=(), dtype=float32)\n1827.9337\ntf.Tensor(1827.9337, shape=(), dtype=float32)\n1827.7178\ntf.Tensor(1827.7178, shape=(), dtype=float32)\n1827.8545\ntf.Tensor(1827.8545, shape=(), dtype=float32)\n1827.8269\ntf.Tensor(1827.8269, shape=(), dtype=float32)\n1827.7886\ntf.Tensor(1827.7886, shape=(), dtype=float32)\n1827.9224\ntf.Tensor(1827.9224, shape=(), dtype=float32)\n1827.9291\ntf.Tensor(1827.9291, shape=(), dtype=float32)\n1827.7258\ntf.Tensor(1827.7258, shape=(), dtype=float32)\n1828.2859\ntf.Tensor(1828.2859, shape=(), dtype=float32)\n1827.8766\ntf.Tensor(1827.8766, shape=(), dtype=float32)\n1827.7247\ntf.Tensor(1827.7247, shape=(), dtype=float32)\n1827.7885\ntf.Tensor(1827.7885, shape=(), dtype=float32)\n1827.389\ntf.Tensor(1827.389, shape=(), dtype=float32)\n1828.1954\ntf.Tensor(1828.1954, shape=(), dtype=float32)\n1827.7809\ntf.Tensor(1827.7809, shape=(), dtype=float32)\n1827.7222\ntf.Tensor(1827.7222, shape=(), dtype=float32)\n1827.8356\ntf.Tensor(1827.8356, shape=(), dtype=float32)\n1827.9047\ntf.Tensor(1827.9047, shape=(), dtype=float32)\n1827.9048\ntf.Tensor(1827.9048, shape=(), dtype=float32)\n1827.8981\ntf.Tensor(1827.8981, shape=(), dtype=float32)\n1827.8784\ntf.Tensor(1827.8784, shape=(), dtype=float32)\n1827.8679\ntf.Tensor(1827.8679, shape=(), dtype=float32)\n","output_type":"stream"}]}]}