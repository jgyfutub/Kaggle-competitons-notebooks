{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1926230,"sourceType":"datasetVersion","datasetId":1148896}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nimport numpy as np\nimport re\ndataset=pd.read_csv('../input/en-fr-translation-dataset/en-fr.csv')\ndataset.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-28T15:03:30.464906Z","iopub.execute_input":"2024-05-28T15:03:30.465360Z","iopub.status.idle":"2024-05-28T15:08:57.884735Z","shell.execute_reply.started":"2024-05-28T15:03:30.465324Z","shell.execute_reply":"2024-05-28T15:08:57.882896Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-05-28 15:03:34.650303: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-28 15:03:34.650416: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-28 15:03:34.838979: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"                                                  en  \\\n0  Changing Lives | Changing Society | How It Wor...   \n1                                           Site map   \n2                                           Feedback   \n3                                            Credits   \n4                                           Français   \n\n                                                  fr  \n0  Il a transformé notre vie | Il a transformé la...  \n1                                       Plan du site  \n2                                        Rétroaction  \n3                                            Crédits  \n4                                            English  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>fr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Changing Lives | Changing Society | How It Wor...</td>\n      <td>Il a transformé notre vie | Il a transformé la...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Site map</td>\n      <td>Plan du site</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Feedback</td>\n      <td>Rétroaction</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Credits</td>\n      <td>Crédits</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Français</td>\n      <td>English</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def english_preprocessing(data , col) : \n    data[col] = data[col].astype(str) \n    data[col] = data[col].apply(lambda x: x.lower())\n    data[col] = data[col].apply(lambda x: re.sub(\"[^A-Za-z\\s]\",\"\",x)) \n    data[col] = data[col].apply(lambda x: x.replace(\"\\s+\",\" \"))\n    data[col] = data[col].apply(lambda x: \" \".join([word for word in x.split()]))\n    return data \ndef french_preprocessing(data , col) : \n    data[col] = data[col].astype(str) \n    data[col] = data[col].apply(lambda x : x.lower()) \n    data[col] = data[col].apply(lambda x: re.sub(r'\\d','',x))\n    data[col] = data[col].apply(lambda x: re.sub(r'\\s+',' ',x))\n    data[col] = data[col].apply(lambda x: re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,।]\", \"\", x))\n    data[col] = data[col].apply(lambda x: x.strip()) \n    data[col] = \"<sos> \" + data[col] + \" <eos>\" \n    return data","metadata":{"execution":{"iopub.status.busy":"2024-05-28T15:08:57.887824Z","iopub.execute_input":"2024-05-28T15:08:57.888267Z","iopub.status.idle":"2024-05-28T15:08:57.902689Z","shell.execute_reply.started":"2024-05-28T15:08:57.888222Z","shell.execute_reply":"2024-05-28T15:08:57.901443Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from collections import Counter \ndef tokenizer(col):\n    if col=='en':\n        sents = english_preprocessing(dataset[:100] , col)[col].tolist()  \n    elif col=='fr':\n         sents = french_preprocessing(dataset[:100] , col)[col].tolist()  \n    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=100 , oov_token = \"<OOV>\" , filters='!#$%&()*+,-/:;<=>@«»\"\"[\\\\]^_`{|}~\\t\\n')\n    tokenizer.fit_on_texts(sents) \n    tokenizer.word_index['<pad>'] = 0 \n    tokenizer.index_word[0] = '<pad>' \n    vocab_to_idx = tokenizer.word_index \n    idx_to_vocab = tokenizer.index_word \n    seqs = tokenizer.texts_to_sequences(sents)  \n    pad_seqs = tf.keras.preprocessing.sequence.pad_sequences(seqs , maxlen =100 , padding='post')\n    return vocab_to_idx , idx_to_vocab , pad_seqs , tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-05-28T15:08:57.904611Z","iopub.execute_input":"2024-05-28T15:08:57.905367Z","iopub.status.idle":"2024-05-28T15:08:57.917483Z","shell.execute_reply.started":"2024-05-28T15:08:57.905322Z","shell.execute_reply":"2024-05-28T15:08:57.916052Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"en_vocab , en_inv_vocab , en_seqs , en_tokenizer = tokenizer('en')\nfr_vocab , fr_inv_vocab , fr_seqs , fr_tokenizer = tokenizer('fr')","metadata":{"execution":{"iopub.status.busy":"2024-05-28T15:08:57.921250Z","iopub.execute_input":"2024-05-28T15:08:57.921799Z","iopub.status.idle":"2024-05-28T15:08:57.964894Z","shell.execute_reply.started":"2024-05-28T15:08:57.921748Z","shell.execute_reply":"2024-05-28T15:08:57.963262Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_33/386876738.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data[col] = data[col].astype(str)\n/tmp/ipykernel_33/386876738.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data[col] = data[col].apply(lambda x: x.lower())\n/tmp/ipykernel_33/386876738.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data[col] = data[col].apply(lambda x: re.sub(\"[^A-Za-z\\s]\",\"\",x))\n/tmp/ipykernel_33/386876738.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data[col] = data[col].apply(lambda x: x.replace(\"\\s+\",\" \"))\n/tmp/ipykernel_33/386876738.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data[col] = data[col].apply(lambda x: \" \".join([word for word in x.split()]))\n/tmp/ipykernel_33/386876738.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data[col] = data[col].astype(str)\n/tmp/ipykernel_33/386876738.py:10: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data[col] = data[col].apply(lambda x : x.lower())\n/tmp/ipykernel_33/386876738.py:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data[col] = data[col].apply(lambda x: re.sub(r'\\d','',x))\n/tmp/ipykernel_33/386876738.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data[col] = data[col].apply(lambda x: re.sub(r'\\s+',' ',x))\n/tmp/ipykernel_33/386876738.py:13: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data[col] = data[col].apply(lambda x: re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,।]\", \"\", x))\n/tmp/ipykernel_33/386876738.py:14: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data[col] = data[col].apply(lambda x: x.strip())\n/tmp/ipykernel_33/386876738.py:15: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data[col] = \"<sos> \" + data[col] + \" <eos>\"\n","output_type":"stream"}]},{"cell_type":"code","source":"model = tf.keras.Sequential([ tf.keras.layers.Embedding(input_dim=10000 ,output_dim=768, input_length=100)])\ndef embedder(text):\n    global model\n    cls_embedding = model(text)\n    #Positional encoding \n    seq_len,d,n=100,768,10000\n    P = np.zeros((seq_len, d))\n    for k in range(seq_len):\n        for i in np.arange(int(d/2)):\n            denominator = np.power(n, 2*i/d)\n            P[k, 2*i] = np.sin(k/denominator)\n            P[k, 2*i+1] = np.cos(k/denominator)\n    #Adding positional encoding\n    cls_embedding += P\n    return tf.expand_dims(cls_embedding, axis=0)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-28T15:08:57.966267Z","iopub.execute_input":"2024-05-28T15:08:57.966597Z","iopub.status.idle":"2024-05-28T15:08:57.990817Z","shell.execute_reply.started":"2024-05-28T15:08:57.966569Z","shell.execute_reply":"2024-05-28T15:08:57.989236Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:86: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"#encoder\nWqe=tf.random.normal(shape=(1,100, 768))\nWke=tf.random.normal(shape=(1,100, 768))\nWve=tf.random.normal(shape=(1,100, 768))\nW0e=tf.random.normal(shape=(1,100, 768))\nW1e=tf.random.normal(shape=(1,100, 768))\n#decoder\nWqd=tf.random.normal(shape=(1,100, 768))\nWkd=tf.random.normal(shape=(1,100, 768))\nWvd=tf.random.normal(shape=(1,100 ,768))\nW0d=tf.random.normal(shape=(1,100 ,768))\nW1d=tf.random.normal(shape=(1,100 ,768))\nW01d=tf.random.normal(shape=(1,100, 768))\nW11d=tf.random.normal(shape=(1,100, 768))\n#train\nW0t=tf.random.normal(shape=(1,100, 768))","metadata":{"execution":{"iopub.status.busy":"2024-05-28T15:08:57.992782Z","iopub.execute_input":"2024-05-28T15:08:57.993297Z","iopub.status.idle":"2024-05-28T15:08:58.159861Z","shell.execute_reply.started":"2024-05-28T15:08:57.993255Z","shell.execute_reply":"2024-05-28T15:08:58.158477Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import math\ndef encoder(embed):\n    # weights of query ,keys and values and other weights\n    global Wqe,Wke,Wve,W0e,W1e\n    #Calculating query ,keys and values\n    Query=embed*Wqe\n    Key=embed*Wke\n    Value=embed*Wve\n    # Scaled-Dot Product Attention\n    scores = tf.matmul(Query, Key, transpose_b=True) / math.sqrt(tf.cast(768, tf.float32))\n    weights = tf.keras.activations.softmax(scores)\n    result=tf.matmul(weights, Value)\n    #Add layer\n    embed+=result\n    #normalize the embeddings\n    normalized_embeddings = tf.keras.layers.LayerNormalization()(embed)\n    #feed forward layer from scratch\n    Layer1=W0e*normalized_embeddings\n    result1=tf.nn.relu(Layer1)\n    result=W1e*normalized_embeddings\n    #Add layer\n    embed+=result\n    #normalize the embeddings\n    normalized_embeddings = tf.keras.layers.LayerNormalization()(embed)\n    Key=normalized_embeddings*Wke\n    Value=normalized_embeddings*Wve\n    return [Key,Value]","metadata":{"execution":{"iopub.status.busy":"2024-05-28T15:08:58.161900Z","iopub.execute_input":"2024-05-28T15:08:58.162422Z","iopub.status.idle":"2024-05-28T15:08:58.173395Z","shell.execute_reply.started":"2024-05-28T15:08:58.162367Z","shell.execute_reply":"2024-05-28T15:08:58.171921Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def decoder(embed, arr):\n    # weights of query ,keys and values and other weights\n    global Wqd,Wkd,Wvd,W0d,W1d,W01d,W11d\n    #Calculating query ,keys and values\n    Query=embed*Wqd\n    Key=embed*Wkd\n    Value=embed*Wvd\n    # Scaled-Dot Product Attention\n    scores = tf.matmul(Query, Key, transpose_b=True) / math.sqrt(tf.cast(768, tf.float32))\n    weights = tf.keras.activations.softmax(scores)\n    result=tf.matmul(weights, Value)\n    #Add layer\n    embed+=result\n    #normalize the embeddings\n    normalized_embeddings = tf.keras.layers.LayerNormalization()(embed)\n    #feed forward layer from scratch\n    Layer1=W0d*normalized_embeddings\n    result1=tf.nn.relu(Layer1)\n    result=W1d*normalized_embeddings\n    #Add layer\n    embed+=result\n    #normalize the embeddings\n    embed = tf.keras.layers.LayerNormalization()(embed)\n    #key ,value of encodings output\n    Key,Value=arr\n    Query=embed*Wqd\n    scores = tf.matmul(Query, Key, transpose_b=True) / math.sqrt(tf.cast(768, tf.float32))\n    weights = tf.keras.activations.softmax(scores)\n    result=tf.matmul(weights, Value)\n    #Add layer\n    embed+=result\n    #normalize the embeddings\n    normalized_embeddings = tf.keras.layers.LayerNormalization()(embed)\n    #feed forward layer from scratch\n    Layer1=W01d*normalized_embeddings\n    result1=tf.nn.relu(Layer1)\n    result=W11d*normalized_embeddings\n    #Add layer\n    embed+=result\n    return embed","metadata":{"execution":{"iopub.status.busy":"2024-05-28T15:08:58.175052Z","iopub.execute_input":"2024-05-28T15:08:58.175435Z","iopub.status.idle":"2024-05-28T15:08:58.189692Z","shell.execute_reply.started":"2024-05-28T15:08:58.175406Z","shell.execute_reply":"2024-05-28T15:08:58.188440Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def train(src , trg,opt):\n    global model,tokenizer,W0t,Wqe,Wke,Wve,W0e,W1e,Wqd,Wkd,Wvd,W0d,W1d,W01d,W11d\n    encoder_output=encoder(embedder(src))\n    output_embed=embedder(trg)\n    decoder_output=decoder(output_embed,encoder_output)\n    Layer1=W0t*decoder_output\n    result=tf.nn.softmax(Layer1, axis=-1)\n    predicted_id = tf.cast(tf.argmax(result, axis=-1), tf.int64) \n    pred_sent = ' '.join([fr_tokenizer.index_word[idx] for idx in predicted_id[0].numpy() if idx != 0 and idx != 2 and idx !=3 and idx in fr_tokenizer.index_word.keys() ])\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=output_embed, logits=result))\n    return loss.numpy()","metadata":{"execution":{"iopub.status.busy":"2024-05-28T15:08:58.191692Z","iopub.execute_input":"2024-05-28T15:08:58.192040Z","iopub.status.idle":"2024-05-28T15:08:58.206439Z","shell.execute_reply.started":"2024-05-28T15:08:58.192011Z","shell.execute_reply":"2024-05-28T15:08:58.204963Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_set = tf.data.Dataset.from_tensor_slices((en_seqs , fr_seqs ))\ntrain_set = train_set.shuffle(100).batch(15 , drop_remainder = True)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T15:08:58.209933Z","iopub.execute_input":"2024-05-28T15:08:58.210408Z","iopub.status.idle":"2024-05-28T15:08:58.247952Z","shell.execute_reply.started":"2024-05-28T15:08:58.210372Z","shell.execute_reply":"2024-05-28T15:08:58.246667Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm \ndef trainloop(EPOCHS,optimizer):\n    global W0t,Wqe,Wke,Wve,W0e,W1e,Wqd,Wkd,Wvd,W0d,W1d,W01d,W11d\n    for epoch in tqdm(range(EPOCHS)) :\n        print(f'epoch : {epoch}')\n        lossofepoch=0\n        s=0\n        for src , trg in tqdm(train_set) : \n            for i in range(15):\n                loss=train(src[i],trg[i],optimizer)\n                #backpropagation\n                W0t+=1e-6\n                Wqe+=1e-6\n                Wke+=1e-6\n                Wve+=1e-6\n                W0e+=1e-6\n                W1e+=1e-6\n                Wqd+=1e-6\n                Wkd+=1e-6\n                Wvd+=1e-6\n                W0d+=1e-6\n                W1d+=1e-6\n                W01d+=1e-6\n                W11d+=1e-6\n                loss1=train(src[i],trg[i],optimizer)\n                gradient = (loss1-loss) / 1e-6\n                W0t-=gradient*optimizer\n                Wqe-=gradient*optimizer\n                Wke-=gradient*optimizer\n                Wve-=gradient*optimizer\n                W0e-=gradient*optimizer\n                W1e-=gradient*optimizer\n                Wqd-=gradient*optimizer\n                Wkd-=gradient*optimizer\n                Wvd-=gradient*optimizer\n                W0d-=gradient*optimizer\n                W1d-=gradient*optimizer\n                W01d-=gradient*optimizer\n                W11d-=gradient*optimizer\n                lossofepoch+=loss\n                s+=1\n            print(lossofepoch/s)\ntrainloop(2,0.0001)","metadata":{"execution":{"iopub.status.busy":"2024-05-28T15:08:58.249469Z","iopub.execute_input":"2024-05-28T15:08:58.249819Z","iopub.status.idle":"2024-05-28T15:13:39.236904Z","shell.execute_reply.started":"2024-05-28T15:08:58.249790Z","shell.execute_reply":"2024-05-28T15:13:39.235338Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bb46f5b97c645a2b26e67276037bc76"}},"metadata":{}},{"name":"stdout","text":"epoch : 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"021b6d0f248a4e0eaca49ef7b18834d3"}},"metadata":{}},{"name":"stdout","text":"1827.0266682942708\n1827.0053385416666\n1827.0487250434028\n1827.0623392740886\n1827.0573714192708\n1827.0533230251735\nepoch : 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67b1cb27599144bf835b7ea6f4b3d1fe"}},"metadata":{}},{"name":"stdout","text":"1827.0019205729166\n1827.0078653971354\n1826.997696940104\n1826.9637451171875\n1826.9264583333334\n1826.9295369466147\n","output_type":"stream"}]},{"cell_type":"code","source":"def test(src):\n    length=len([i for i in src if i!=0])\n    t='sos'\n    print(length)\n    for i in range(length):\n        decoder_input = fr_tokenizer.texts_to_sequences([t])\n        decoder_input= tf.keras.preprocessing.sequence.pad_sequences(decoder_input , maxlen =100 , padding='post')\n        decoder_input = tf.convert_to_tensor(np.array(decoder_input) , dtype = tf.int64)[0]\n        encoder_output=encoder(embedder(src))\n        output_embed=embedder(decoder_input)\n        decoder_output=decoder(output_embed,encoder_output)\n        Layer1=W0t*decoder_output\n        result=tf.nn.softmax(Layer1, axis=-1)\n        predicted_id = max(tf.cast(tf.argmax(result, axis=-1), tf.int64)[0])\n        predicted_id=tf.reduce_mean(tf.cast(tf.argmax(result, axis=-1), tf.int64))\n        if predicted_id.numpy() in fr_tokenizer.index_word.keys():\n            t+=\" \"+fr_tokenizer.index_word[predicted_id.numpy()] +\" \"\n        else:\n            t+=\" <PAD> \"\n    return t\ntest(en_seqs[40])","metadata":{"execution":{"iopub.status.busy":"2024-05-28T16:29:28.829420Z","iopub.execute_input":"2024-05-28T16:29:28.829900Z","iopub.status.idle":"2024-05-28T16:29:39.183257Z","shell.execute_reply.started":"2024-05-28T16:29:28.829864Z","shell.execute_reply":"2024-05-28T16:29:39.182273Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"13\n","output_type":"stream"},{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"\"sos observant  observant  c'est  c'est  c'est  c'est  c'est  c'est  c'est  c'est  c'est  c'est  c'est \""},"metadata":{}}]}]}